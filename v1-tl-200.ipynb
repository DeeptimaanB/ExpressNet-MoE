{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de57fb8-f88c-4674-b3d6-b383ef2d9e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 14:23:33.212077: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 14:23:33.815275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a41117-6039-467d-a39c-ae54ad725af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'FER-2013/test'  \n",
    "train_dir = 'FER-2013/train'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a80a85-8a36-43c9-a4c6-485f3d243f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    brightness_range =[0.8,1.2],\n",
    "    shear_range = 0.15,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7bb2010-7e1e-4f4a-a466-877b9fcad21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen_train.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),    # FER images are typically 48x48\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = datagen_val.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43be9da2-cff2-46cf-8291-c50496a74470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 14:23:36.219799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11541 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:19:00.0, compute capability: 6.1\n",
      "2024-11-21 14:23:36.220360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 11541 MB memory:  -> device: 1, name: NVIDIA TITAN Xp, pci bus id: 0000:1a:00.0, compute capability: 6.1\n",
      "2024-11-21 14:23:36.220868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 11541 MB memory:  -> device: 2, name: NVIDIA TITAN Xp, pci bus id: 0000:67:00.0, compute capability: 6.1\n",
      "2024-11-21 14:23:36.221364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 11533 MB memory:  -> device: 3, name: NVIDIA TITAN Xp, pci bus id: 0000:68:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 256)               1024      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14881351 (56.77 MB)\n",
      "Trainable params: 7245319 (27.64 MB)\n",
      "Non-trainable params: 7636032 (29.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Load VGG16 pre-trained on ImageNet, without the top fully connected layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "\n",
    "# Freeze all layers except the last 3\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Define the emotion detection model\n",
    "model = Sequential([\n",
    "    base_model,  # Pre-trained VGG16 as the base\n",
    "    GlobalAveragePooling2D(),  # Replace Flatten with GAP\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(7, activation='softmax')  # 7 classes for emotion detection\n",
    "])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac3c847-6515-4fe4-aee5-cdc594f3ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN\n",
    "# model = Sequential([\n",
    "#     Conv2D(256, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "#     BatchNormalization(),\n",
    "#     MaxPooling2D(2, 2),\n",
    "#     Dropout(0.3),\n",
    "\n",
    "#     Conv2D(128, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "#     MaxPooling2D(2, 2),\n",
    "#     Dropout(0.3),\n",
    "\n",
    "#     Conv2D(64, (3, 3), activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "#     MaxPooling2D(2, 2),\n",
    "#     Dropout(0.3),\n",
    "\n",
    "#     Flatten(),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(7, activation='softmax')  # Adjust if there are more/less classes\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c1532a1-9105-4b9c-91c3-bbcf65c68b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9530eb-005b-4c66-b528-ab796a8b5ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 256)               1024      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14881351 (56.77 MB)\n",
      "Trainable params: 7245319 (27.64 MB)\n",
      "Non-trainable params: 7636032 (29.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5c2c0-2f42-4bed-80ab-f4a5a99aa32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 14:23:38.305357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-11-21 14:23:38.778365: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8b38162fd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-21 14:23:38.778392: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA TITAN Xp, Compute Capability 6.1\n",
      "2024-11-21 14:23:38.778397: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA TITAN Xp, Compute Capability 6.1\n",
      "2024-11-21 14:23:38.778400: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA TITAN Xp, Compute Capability 6.1\n",
      "2024-11-21 14:23:38.778404: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA TITAN Xp, Compute Capability 6.1\n",
      "2024-11-21 14:23:38.781729: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-21 14:23:38.796235: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:543] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-11.8\n",
      "  /usr/local/cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2024-11-21 14:23:38.893517: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/449 [==============================] - 39s 80ms/step - loss: 3.4506 - accuracy: 0.2528 - val_loss: 2.3688 - val_accuracy: 0.3160\n",
      "Epoch 2/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 2.0993 - accuracy: 0.3159 - val_loss: 3.0732 - val_accuracy: 0.2136\n",
      "Epoch 3/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.8025 - accuracy: 0.3603 - val_loss: 1.8778 - val_accuracy: 0.3415\n",
      "Epoch 4/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.6751 - accuracy: 0.3851 - val_loss: 1.7070 - val_accuracy: 0.3885\n",
      "Epoch 5/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.6101 - accuracy: 0.4057 - val_loss: 1.6682 - val_accuracy: 0.4213\n",
      "Epoch 6/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.5776 - accuracy: 0.4129 - val_loss: 1.4678 - val_accuracy: 0.4606\n",
      "Epoch 7/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.5537 - accuracy: 0.4202 - val_loss: 1.5469 - val_accuracy: 0.4250\n",
      "Epoch 8/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.5355 - accuracy: 0.4243 - val_loss: 1.4895 - val_accuracy: 0.4514\n",
      "Epoch 9/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.5192 - accuracy: 0.4332 - val_loss: 1.4952 - val_accuracy: 0.4507\n",
      "Epoch 10/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.5246 - accuracy: 0.4326 - val_loss: 1.5708 - val_accuracy: 0.4298\n",
      "Epoch 11/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.5034 - accuracy: 0.4369 - val_loss: 1.4303 - val_accuracy: 0.4671\n",
      "Epoch 12/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.4913 - accuracy: 0.4455 - val_loss: 1.4276 - val_accuracy: 0.4685\n",
      "Epoch 13/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.4886 - accuracy: 0.4433 - val_loss: 1.3674 - val_accuracy: 0.4877\n",
      "Epoch 14/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.4738 - accuracy: 0.4477 - val_loss: 1.3670 - val_accuracy: 0.4870\n",
      "Epoch 15/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.4663 - accuracy: 0.4549 - val_loss: 1.4269 - val_accuracy: 0.4617\n",
      "Epoch 16/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.4551 - accuracy: 0.4606 - val_loss: 1.4373 - val_accuracy: 0.4638\n",
      "Epoch 17/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.4481 - accuracy: 0.4620 - val_loss: 1.4366 - val_accuracy: 0.4558\n",
      "Epoch 18/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.4458 - accuracy: 0.4640 - val_loss: 1.3802 - val_accuracy: 0.4812\n",
      "Epoch 19/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.4350 - accuracy: 0.4670 - val_loss: 1.4036 - val_accuracy: 0.4887\n",
      "Epoch 20/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.4351 - accuracy: 0.4691 - val_loss: 1.3822 - val_accuracy: 0.4819\n",
      "Epoch 21/200\n",
      "449/449 [==============================] - 35s 79ms/step - loss: 1.4280 - accuracy: 0.4718 - val_loss: 1.3777 - val_accuracy: 0.4857\n",
      "Epoch 22/200\n",
      "449/449 [==============================] - 37s 83ms/step - loss: 1.4291 - accuracy: 0.4677 - val_loss: 1.3216 - val_accuracy: 0.5088\n",
      "Epoch 23/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.4147 - accuracy: 0.4737 - val_loss: 1.3128 - val_accuracy: 0.5060\n",
      "Epoch 24/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.4057 - accuracy: 0.4773 - val_loss: 1.3175 - val_accuracy: 0.5142\n",
      "Epoch 25/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.4135 - accuracy: 0.4747 - val_loss: 1.3929 - val_accuracy: 0.4855\n",
      "Epoch 26/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.4010 - accuracy: 0.4810 - val_loss: 1.4044 - val_accuracy: 0.4788\n",
      "Epoch 27/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.3996 - accuracy: 0.4813 - val_loss: 1.3160 - val_accuracy: 0.5116\n",
      "Epoch 28/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.3941 - accuracy: 0.4850 - val_loss: 1.2991 - val_accuracy: 0.5187\n",
      "Epoch 29/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.3835 - accuracy: 0.4869 - val_loss: 1.3700 - val_accuracy: 0.4883\n",
      "Epoch 30/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.3837 - accuracy: 0.4904 - val_loss: 1.3536 - val_accuracy: 0.4873\n",
      "Epoch 31/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.3831 - accuracy: 0.4903 - val_loss: 1.3259 - val_accuracy: 0.5099\n",
      "Epoch 32/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.3710 - accuracy: 0.4946 - val_loss: 1.3850 - val_accuracy: 0.4911\n",
      "Epoch 33/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.3712 - accuracy: 0.4959 - val_loss: 1.3032 - val_accuracy: 0.5176\n",
      "Epoch 34/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.3690 - accuracy: 0.4968 - val_loss: 1.2934 - val_accuracy: 0.5217\n",
      "Epoch 35/200\n",
      "449/449 [==============================] - 37s 83ms/step - loss: 1.3607 - accuracy: 0.4960 - val_loss: 1.3507 - val_accuracy: 0.5008\n",
      "Epoch 36/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.3703 - accuracy: 0.4956 - val_loss: 1.2853 - val_accuracy: 0.5238\n",
      "Epoch 37/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.3593 - accuracy: 0.5010 - val_loss: 1.3287 - val_accuracy: 0.5117\n",
      "Epoch 38/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.3579 - accuracy: 0.4984 - val_loss: 1.2811 - val_accuracy: 0.5266\n",
      "Epoch 39/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.3545 - accuracy: 0.5035 - val_loss: 1.2846 - val_accuracy: 0.5295\n",
      "Epoch 40/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.3498 - accuracy: 0.5045 - val_loss: 1.3012 - val_accuracy: 0.5216\n",
      "Epoch 41/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.3418 - accuracy: 0.5043 - val_loss: 1.3087 - val_accuracy: 0.5189\n",
      "Epoch 42/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.3442 - accuracy: 0.5088 - val_loss: 1.3230 - val_accuracy: 0.5042\n",
      "Epoch 43/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.3316 - accuracy: 0.5081 - val_loss: 1.2907 - val_accuracy: 0.5284\n",
      "Epoch 44/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.3388 - accuracy: 0.5056 - val_loss: 1.2682 - val_accuracy: 0.5339\n",
      "Epoch 45/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.3378 - accuracy: 0.5096 - val_loss: 1.2662 - val_accuracy: 0.5343\n",
      "Epoch 46/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.3327 - accuracy: 0.5095 - val_loss: 1.2522 - val_accuracy: 0.5372\n",
      "Epoch 47/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.3299 - accuracy: 0.5101 - val_loss: 1.2933 - val_accuracy: 0.5281\n",
      "Epoch 48/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.3203 - accuracy: 0.5148 - val_loss: 1.4625 - val_accuracy: 0.4726\n",
      "Epoch 49/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.3208 - accuracy: 0.5163 - val_loss: 1.2737 - val_accuracy: 0.5323\n",
      "Epoch 50/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.3217 - accuracy: 0.5154 - val_loss: 1.3058 - val_accuracy: 0.5174\n",
      "Epoch 51/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.3217 - accuracy: 0.5175 - val_loss: 1.2538 - val_accuracy: 0.5396\n",
      "Epoch 52/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.3182 - accuracy: 0.5141 - val_loss: 1.2650 - val_accuracy: 0.5281\n",
      "Epoch 53/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.3142 - accuracy: 0.5205 - val_loss: 1.2709 - val_accuracy: 0.5396\n",
      "Epoch 54/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.3077 - accuracy: 0.5220 - val_loss: 1.2362 - val_accuracy: 0.5493\n",
      "Epoch 55/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.3050 - accuracy: 0.5217 - val_loss: 1.2683 - val_accuracy: 0.5305\n",
      "Epoch 56/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.3033 - accuracy: 0.5222 - val_loss: 1.3099 - val_accuracy: 0.5233\n",
      "Epoch 57/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.3043 - accuracy: 0.5229 - val_loss: 1.3634 - val_accuracy: 0.5195\n",
      "Epoch 58/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.3066 - accuracy: 0.5206 - val_loss: 1.3136 - val_accuracy: 0.5176\n",
      "Epoch 59/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2981 - accuracy: 0.5234 - val_loss: 1.2975 - val_accuracy: 0.5247\n",
      "Epoch 60/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.2928 - accuracy: 0.5283 - val_loss: 1.3053 - val_accuracy: 0.5228\n",
      "Epoch 61/200\n",
      "449/449 [==============================] - 37s 81ms/step - loss: 1.3029 - accuracy: 0.5242 - val_loss: 1.2968 - val_accuracy: 0.5262\n",
      "Epoch 62/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2876 - accuracy: 0.5286 - val_loss: 1.2944 - val_accuracy: 0.5293\n",
      "Epoch 63/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2908 - accuracy: 0.5243 - val_loss: 1.2855 - val_accuracy: 0.5322\n",
      "Epoch 64/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.2823 - accuracy: 0.5309 - val_loss: 1.2550 - val_accuracy: 0.5368\n",
      "Epoch 65/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2815 - accuracy: 0.5326 - val_loss: 1.2436 - val_accuracy: 0.5451\n",
      "Epoch 66/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.2848 - accuracy: 0.5314 - val_loss: 1.2631 - val_accuracy: 0.5414\n",
      "Epoch 67/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2796 - accuracy: 0.5313 - val_loss: 1.2533 - val_accuracy: 0.5419\n",
      "Epoch 68/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2782 - accuracy: 0.5357 - val_loss: 1.2344 - val_accuracy: 0.5457\n",
      "Epoch 69/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2793 - accuracy: 0.5318 - val_loss: 1.2543 - val_accuracy: 0.5472\n",
      "Epoch 70/200\n",
      "449/449 [==============================] - 35s 79ms/step - loss: 1.2768 - accuracy: 0.5334 - val_loss: 1.2447 - val_accuracy: 0.5410\n",
      "Epoch 71/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.2762 - accuracy: 0.5331 - val_loss: 1.2395 - val_accuracy: 0.5495\n",
      "Epoch 72/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2749 - accuracy: 0.5374 - val_loss: 1.2482 - val_accuracy: 0.5435\n",
      "Epoch 73/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2700 - accuracy: 0.5373 - val_loss: 1.3993 - val_accuracy: 0.5021\n",
      "Epoch 74/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.2606 - accuracy: 0.5376 - val_loss: 1.2439 - val_accuracy: 0.5435\n",
      "Epoch 75/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.2629 - accuracy: 0.5375 - val_loss: 1.2616 - val_accuracy: 0.5336\n",
      "Epoch 76/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.2642 - accuracy: 0.5367 - val_loss: 1.2408 - val_accuracy: 0.5481\n",
      "Epoch 77/200\n",
      "449/449 [==============================] - 35s 78ms/step - loss: 1.2621 - accuracy: 0.5358 - val_loss: 1.2290 - val_accuracy: 0.5506\n",
      "Epoch 78/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2549 - accuracy: 0.5428 - val_loss: 1.2523 - val_accuracy: 0.5373\n",
      "Epoch 79/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2563 - accuracy: 0.5409 - val_loss: 1.2144 - val_accuracy: 0.5581\n",
      "Epoch 80/200\n",
      "449/449 [==============================] - 37s 81ms/step - loss: 1.2481 - accuracy: 0.5442 - val_loss: 1.2586 - val_accuracy: 0.5387\n",
      "Epoch 81/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.2533 - accuracy: 0.5418 - val_loss: 1.2474 - val_accuracy: 0.5478\n",
      "Epoch 82/200\n",
      "449/449 [==============================] - 35s 79ms/step - loss: 1.2569 - accuracy: 0.5423 - val_loss: 1.2366 - val_accuracy: 0.5496\n",
      "Epoch 83/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.2485 - accuracy: 0.5459 - val_loss: 1.2888 - val_accuracy: 0.5421\n",
      "Epoch 84/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.2420 - accuracy: 0.5466 - val_loss: 1.2522 - val_accuracy: 0.5375\n",
      "Epoch 85/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.2395 - accuracy: 0.5486 - val_loss: 1.2346 - val_accuracy: 0.5532\n",
      "Epoch 86/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2448 - accuracy: 0.5470 - val_loss: 1.2657 - val_accuracy: 0.5372\n",
      "Epoch 87/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.2356 - accuracy: 0.5451 - val_loss: 1.2267 - val_accuracy: 0.5503\n",
      "Epoch 88/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2388 - accuracy: 0.5482 - val_loss: 1.2531 - val_accuracy: 0.5464\n",
      "Epoch 89/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.2330 - accuracy: 0.5506 - val_loss: 1.2345 - val_accuracy: 0.5531\n",
      "Epoch 90/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2309 - accuracy: 0.5532 - val_loss: 1.2363 - val_accuracy: 0.5454\n",
      "Epoch 91/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2269 - accuracy: 0.5520 - val_loss: 1.2324 - val_accuracy: 0.5577\n",
      "Epoch 92/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.2257 - accuracy: 0.5521 - val_loss: 1.2874 - val_accuracy: 0.5456\n",
      "Epoch 93/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.2256 - accuracy: 0.5536 - val_loss: 1.2545 - val_accuracy: 0.5442\n",
      "Epoch 94/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2240 - accuracy: 0.5554 - val_loss: 1.2477 - val_accuracy: 0.5449\n",
      "Epoch 95/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2205 - accuracy: 0.5601 - val_loss: 1.2283 - val_accuracy: 0.5563\n",
      "Epoch 96/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2140 - accuracy: 0.5590 - val_loss: 1.2507 - val_accuracy: 0.5541\n",
      "Epoch 97/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.2180 - accuracy: 0.5574 - val_loss: 1.2223 - val_accuracy: 0.5550\n",
      "Epoch 98/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.2138 - accuracy: 0.5569 - val_loss: 1.2390 - val_accuracy: 0.5528\n",
      "Epoch 99/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2133 - accuracy: 0.5588 - val_loss: 1.2161 - val_accuracy: 0.5616\n",
      "Epoch 100/200\n",
      "449/449 [==============================] - 35s 78ms/step - loss: 1.2160 - accuracy: 0.5592 - val_loss: 1.2418 - val_accuracy: 0.5500\n",
      "Epoch 101/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2144 - accuracy: 0.5576 - val_loss: 1.2250 - val_accuracy: 0.5574\n",
      "Epoch 102/200\n",
      "449/449 [==============================] - 35s 78ms/step - loss: 1.2046 - accuracy: 0.5607 - val_loss: 1.3082 - val_accuracy: 0.5366\n",
      "Epoch 103/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2007 - accuracy: 0.5659 - val_loss: 1.2363 - val_accuracy: 0.5550\n",
      "Epoch 104/200\n",
      "449/449 [==============================] - 35s 78ms/step - loss: 1.2014 - accuracy: 0.5637 - val_loss: 1.2498 - val_accuracy: 0.5456\n",
      "Epoch 105/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.2036 - accuracy: 0.5626 - val_loss: 1.2977 - val_accuracy: 0.5449\n",
      "Epoch 106/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1984 - accuracy: 0.5667 - val_loss: 1.2783 - val_accuracy: 0.5513\n",
      "Epoch 107/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.2025 - accuracy: 0.5656 - val_loss: 1.2325 - val_accuracy: 0.5514\n",
      "Epoch 108/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.1950 - accuracy: 0.5645 - val_loss: 1.2113 - val_accuracy: 0.5635\n",
      "Epoch 109/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1891 - accuracy: 0.5678 - val_loss: 1.2432 - val_accuracy: 0.5550\n",
      "Epoch 110/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1866 - accuracy: 0.5702 - val_loss: 1.2518 - val_accuracy: 0.5549\n",
      "Epoch 111/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1916 - accuracy: 0.5684 - val_loss: 1.2155 - val_accuracy: 0.5627\n",
      "Epoch 112/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1914 - accuracy: 0.5693 - val_loss: 1.2536 - val_accuracy: 0.5524\n",
      "Epoch 113/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.1836 - accuracy: 0.5700 - val_loss: 1.2603 - val_accuracy: 0.5582\n",
      "Epoch 114/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1848 - accuracy: 0.5711 - val_loss: 1.2869 - val_accuracy: 0.5432\n",
      "Epoch 115/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1839 - accuracy: 0.5714 - val_loss: 1.2193 - val_accuracy: 0.5652\n",
      "Epoch 116/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.1765 - accuracy: 0.5740 - val_loss: 1.2264 - val_accuracy: 0.5610\n",
      "Epoch 117/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.1753 - accuracy: 0.5763 - val_loss: 1.2515 - val_accuracy: 0.5539\n",
      "Epoch 118/200\n",
      "449/449 [==============================] - 37s 81ms/step - loss: 1.1790 - accuracy: 0.5726 - val_loss: 1.2309 - val_accuracy: 0.5592\n",
      "Epoch 119/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.1813 - accuracy: 0.5745 - val_loss: 1.3279 - val_accuracy: 0.5249\n",
      "Epoch 120/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1778 - accuracy: 0.5742 - val_loss: 1.2348 - val_accuracy: 0.5536\n",
      "Epoch 121/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.1696 - accuracy: 0.5757 - val_loss: 1.2949 - val_accuracy: 0.5408\n",
      "Epoch 122/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.1696 - accuracy: 0.5791 - val_loss: 1.2295 - val_accuracy: 0.5589\n",
      "Epoch 123/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.1732 - accuracy: 0.5743 - val_loss: 1.2160 - val_accuracy: 0.5691\n",
      "Epoch 124/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.1706 - accuracy: 0.5787 - val_loss: 1.2305 - val_accuracy: 0.5571\n",
      "Epoch 125/200\n",
      "449/449 [==============================] - 34s 77ms/step - loss: 1.1703 - accuracy: 0.5805 - val_loss: 1.2593 - val_accuracy: 0.5556\n",
      "Epoch 126/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.1604 - accuracy: 0.5812 - val_loss: 1.2227 - val_accuracy: 0.5641\n",
      "Epoch 127/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.1667 - accuracy: 0.5798 - val_loss: 1.2500 - val_accuracy: 0.5546\n",
      "Epoch 128/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1595 - accuracy: 0.5793 - val_loss: 1.2161 - val_accuracy: 0.5624\n",
      "Epoch 129/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1582 - accuracy: 0.5827 - val_loss: 1.2340 - val_accuracy: 0.5637\n",
      "Epoch 130/200\n",
      "449/449 [==============================] - 35s 79ms/step - loss: 1.1602 - accuracy: 0.5810 - val_loss: 1.2237 - val_accuracy: 0.5607\n",
      "Epoch 131/200\n",
      "449/449 [==============================] - 37s 81ms/step - loss: 1.1501 - accuracy: 0.5851 - val_loss: 1.2531 - val_accuracy: 0.5531\n",
      "Epoch 132/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.1565 - accuracy: 0.5825 - val_loss: 1.2432 - val_accuracy: 0.5563\n",
      "Epoch 133/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1503 - accuracy: 0.5837 - val_loss: 1.2194 - val_accuracy: 0.5593\n",
      "Epoch 134/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.1408 - accuracy: 0.5907 - val_loss: 1.2317 - val_accuracy: 0.5620\n",
      "Epoch 135/200\n",
      "449/449 [==============================] - 36s 79ms/step - loss: 1.1471 - accuracy: 0.5855 - val_loss: 1.2285 - val_accuracy: 0.5676\n",
      "Epoch 136/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.1429 - accuracy: 0.5893 - val_loss: 1.2616 - val_accuracy: 0.5584\n",
      "Epoch 137/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1418 - accuracy: 0.5898 - val_loss: 1.2517 - val_accuracy: 0.5648\n",
      "Epoch 138/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1425 - accuracy: 0.5881 - val_loss: 1.2615 - val_accuracy: 0.5535\n",
      "Epoch 139/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1506 - accuracy: 0.5859 - val_loss: 1.2628 - val_accuracy: 0.5520\n",
      "Epoch 140/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.1436 - accuracy: 0.5885 - val_loss: 1.2262 - val_accuracy: 0.5705\n",
      "Epoch 141/200\n",
      "449/449 [==============================] - 37s 81ms/step - loss: 1.1397 - accuracy: 0.5901 - val_loss: 1.2245 - val_accuracy: 0.5676\n",
      "Epoch 142/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.1418 - accuracy: 0.5919 - val_loss: 1.2535 - val_accuracy: 0.5587\n",
      "Epoch 143/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1349 - accuracy: 0.5966 - val_loss: 1.2636 - val_accuracy: 0.5596\n",
      "Epoch 144/200\n",
      "449/449 [==============================] - 35s 78ms/step - loss: 1.1317 - accuracy: 0.5935 - val_loss: 1.2367 - val_accuracy: 0.5632\n",
      "Epoch 145/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.1409 - accuracy: 0.5891 - val_loss: 1.2626 - val_accuracy: 0.5493\n",
      "Epoch 146/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.1452 - accuracy: 0.5897 - val_loss: 1.2397 - val_accuracy: 0.5676\n",
      "Epoch 147/200\n",
      "449/449 [==============================] - 35s 79ms/step - loss: 1.1300 - accuracy: 0.5935 - val_loss: 1.2556 - val_accuracy: 0.5613\n",
      "Epoch 148/200\n",
      "449/449 [==============================] - 35s 78ms/step - loss: 1.1278 - accuracy: 0.5960 - val_loss: 1.2274 - val_accuracy: 0.5648\n",
      "Epoch 149/200\n",
      "449/449 [==============================] - 37s 81ms/step - loss: 1.1258 - accuracy: 0.5923 - val_loss: 1.2673 - val_accuracy: 0.5596\n",
      "Epoch 150/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1249 - accuracy: 0.5946 - val_loss: 1.2556 - val_accuracy: 0.5616\n",
      "Epoch 151/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1263 - accuracy: 0.5960 - val_loss: 1.2921 - val_accuracy: 0.5584\n",
      "Epoch 152/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1224 - accuracy: 0.5975 - val_loss: 1.2242 - val_accuracy: 0.5720\n",
      "Epoch 153/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1165 - accuracy: 0.5968 - val_loss: 1.2480 - val_accuracy: 0.5580\n",
      "Epoch 154/200\n",
      "449/449 [==============================] - 37s 81ms/step - loss: 1.1262 - accuracy: 0.5965 - val_loss: 1.2165 - val_accuracy: 0.5711\n",
      "Epoch 155/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.1110 - accuracy: 0.6033 - val_loss: 1.2506 - val_accuracy: 0.5635\n",
      "Epoch 156/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1179 - accuracy: 0.5995 - val_loss: 1.3137 - val_accuracy: 0.5415\n",
      "Epoch 157/200\n",
      "449/449 [==============================] - 35s 79ms/step - loss: 1.1099 - accuracy: 0.5973 - val_loss: 1.2627 - val_accuracy: 0.5582\n",
      "Epoch 158/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.1068 - accuracy: 0.6005 - val_loss: 1.2237 - val_accuracy: 0.5670\n",
      "Epoch 159/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.1170 - accuracy: 0.5989 - val_loss: 1.2511 - val_accuracy: 0.5642\n",
      "Epoch 160/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1056 - accuracy: 0.6075 - val_loss: 1.3053 - val_accuracy: 0.5497\n",
      "Epoch 161/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1134 - accuracy: 0.6005 - val_loss: 1.2604 - val_accuracy: 0.5588\n",
      "Epoch 162/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.1071 - accuracy: 0.6048 - val_loss: 1.3422 - val_accuracy: 0.5514\n",
      "Epoch 163/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1079 - accuracy: 0.6034 - val_loss: 1.2371 - val_accuracy: 0.5669\n",
      "Epoch 164/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1077 - accuracy: 0.6038 - val_loss: 1.2561 - val_accuracy: 0.5663\n",
      "Epoch 165/200\n",
      "449/449 [==============================] - 36s 81ms/step - loss: 1.1029 - accuracy: 0.6066 - val_loss: 1.2487 - val_accuracy: 0.5768\n",
      "Epoch 166/200\n",
      "449/449 [==============================] - 36s 80ms/step - loss: 1.1018 - accuracy: 0.6043 - val_loss: 1.2559 - val_accuracy: 0.5665\n",
      "Epoch 167/200\n",
      "449/449 [==============================] - 37s 82ms/step - loss: 1.0937 - accuracy: 0.6095 - val_loss: 1.2214 - val_accuracy: 0.5709\n",
      "Epoch 168/200\n",
      "137/449 [========>.....................] - ETA: 22s - loss: 1.0728 - accuracy: 0.6217"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, validation_data=val_generator, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bdf892-4d2f-4e80-b533-df7a2625e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"v1-tl.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15d0da-96af-42d2-beb7-5b8541e6d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract data from history\n",
    "history_dict = history.history\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "\n",
    "# Get the number of epochs\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'b-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, accuracy, 'b-', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'r-', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-p39-tf2",
   "language": "python",
   "name": "venv-p39-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
